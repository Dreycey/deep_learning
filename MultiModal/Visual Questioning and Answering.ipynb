{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pickle\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "# image processing\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "# tenserflow imports\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dense, Multiply, Concatenate, Average\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras import Input, Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "The following parameters allow for adjusting the size of the bag-of-words embedding, as well as other various factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_BAGOFWORDS_SIZE = 2000\n",
    "g_EMBIMG_SIZE = 1000 # known from the  RESNET50 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizers for the Questions and Answers.\n",
    "\n",
    "Of note, this is a bag-of-words (BoW) model as seen in Agrawal et al. \n",
    "\n",
    "\n",
    "This was implimented by hand to allow for flecibility in testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionTokenizer:\n",
    "    \"\"\" Question Tokenizer \"\"\"\n",
    "    \n",
    "    def __init__(self, q_string_list, a_string_list, vector_size, percent_q=0.90):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.vector_size = vector_size\n",
    "        self.percent_q = percent_q\n",
    "        self.initialize(q_string_list, a_string_list)\n",
    "        \n",
    "    def tokenize(self, string):\n",
    "        \"\"\"\n",
    "        This method will turn a string of words into \n",
    "        a bag-of-words vector.\n",
    "        \n",
    "        if inherited, use change this.\n",
    "        \"\"\"\n",
    "        token = np.zeros((self.vector_size))\n",
    "        word_list = string.split(\" \")\n",
    "        index_count = {}\n",
    "        for word in word_list:\n",
    "            if word in self.word2index:\n",
    "                index = self.word2index[word]\n",
    "                if index in index_count:\n",
    "                    index_count[index] += 1\n",
    "                else:\n",
    "                    index_count[index] = 1\n",
    "        for index, count in index_count.items():\n",
    "            token[index] = count / max(index_count.values())\n",
    "        return token\n",
    "    \n",
    "    def initialize(self, q_string_list, a_string_list):\n",
    "        \"\"\" \n",
    "        description: creates the underlying datastructure\n",
    "        input: string list for questions, string list for answers\n",
    "        output: count dictionary\n",
    "        \"\"\"\n",
    "        # Obtain words only.\n",
    "        q_word_list = []\n",
    "        for string in q_string_list:\n",
    "            q_word_list += string.split(\" \")\n",
    "        a_word_list = []\n",
    "        for string in a_string_list:\n",
    "            a_word_list += string.split(\" \")\n",
    "        # obtain top words. \n",
    "        # Note: both answers and questions used here due to Agrawal et al. fig 5\n",
    "        print(\"finding top words\")\n",
    "        q_word_top = self.__retrieve_top_N(q_word_list, int(self.vector_size * self.percent_q))\n",
    "        a_word_top = self.__retrieve_top_N(a_word_list, int(self.vector_size * (1-self.percent_q)))\n",
    "        print(\"\\t DONE!\")\n",
    "        \n",
    "        # add to mappings.\n",
    "        print(\"making mappings\")\n",
    "        self.__make_mappings(q_word_top + a_word_top)\n",
    "        print(\"\\t DONE!\")\n",
    "    \n",
    "    def __retrieve_top_N(self, word_list, N):\n",
    "        \"\"\"\n",
    "        description: retrieves the top N words in a count dictionary.\n",
    "        input: number of words to grab, dictionary of word counts.\n",
    "        output: list of top N words.\n",
    "        \"\"\"\n",
    "        # get dictionary count\n",
    "        count_dictionary = self.__count_word_frequency(word_list)\n",
    "        # return top N\n",
    "        tuple_map = [(k,v) for k, v in count_dictionary.items()]\n",
    "        tuple_map.sort(key=lambda a: a[1], reverse=True)\n",
    "        top_N = [i[0] for i in tuple_map[5:5+N]]\n",
    "        return top_N\n",
    "    \n",
    "    def __count_word_frequency(self, string_list):\n",
    "        \"\"\" \n",
    "        description: expects list of strings.\n",
    "        input: string list\n",
    "        output: count dictionary\n",
    "        \"\"\"\n",
    "        word_count = {}\n",
    "        for word in string_list:\n",
    "            if word == '': continue #skip white space\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        return word_count\n",
    "    \n",
    "    def __make_mappings(self, word_list):\n",
    "        \"\"\"\n",
    "        description: given a word list, this method makes mappings\n",
    "        input: creates the vector mappings.\n",
    "        output: None\n",
    "        \"\"\"\n",
    "        for index, word in enumerate(word_list):\n",
    "            self.word2index[word] = index\n",
    "            self.index2word[index] = word\n",
    "            \n",
    "            \n",
    "class AnswerTokenizer(QuestionTokenizer):\n",
    "    \"\"\" Tokenizer for the Answers\"\"\"\n",
    "    \n",
    "    def __init__(self, a_string_list, vector_size, q_string_list=[], percent_q=0.00):\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.vector_size = vector_size\n",
    "        self.percent_q = percent_q\n",
    "        self.initialize(q_string_list, a_string_list)\n",
    "        \n",
    "    def tokenize(self, string):\n",
    "        \"\"\"\n",
    "        This method will turn a string of words into \n",
    "        a TOKENIZED (not bag of words) vector.\n",
    "        \n",
    "        if inherited, use change this.\n",
    "        \"\"\"\n",
    "        token = np.zeros((self.vector_size))\n",
    "        word_list = string.split(\" \")\n",
    "        index_count = {}\n",
    "        for word in word_list:\n",
    "            if word in self.word2index:\n",
    "                index = self.word2index[word]\n",
    "                if index in index_count:\n",
    "                    index_count[index] += 1\n",
    "                else:\n",
    "                    index_count[index] = 1\n",
    "        for index, count in index_count.items():\n",
    "            token[index] = 1\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding top words\n",
      "\t DONE!\n",
      "making mappings\n",
      "\t DONE!\n",
      "{'how': 0}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4f/nzw4pyy13p90pvc1lkbpbjpc0000gn/T/ipykernel_11060/1309271188.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mq_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mq_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'who you'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'you'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_q = [\"who are you\", \"who am I\", \"how are you\"]\n",
    "test_a = [\"no\", \"yes\", \"yes \"]\n",
    "\n",
    "# make tokenizer\n",
    "q_tokenizer = QuestionTokenizer(test_q, test_a, 4)\n",
    "print(q_tokenizer.word2index)\n",
    "\n",
    "# test tokenization\n",
    "q_tokenizer.tokenize('yes')\n",
    "q_tokenizer.tokenize('who you')\n",
    "assert all(q_tokenizer.tokenize('you') == [0., 0., 1., 0.])\n",
    "\n",
    "\n",
    "# test answer tokenization\n",
    "a_tokenizer = AnswerTokenizer(test_a, 2)\n",
    "assert all(a_tokenizer.tokenize('yes no no') == [1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-House Methods\n",
    "\n",
    "1. process_data() - this method preprocesses the input data url, returning vectors ready for the downstream NN.\n",
    "    2. data2rawlists() - this method, used by process_data(), parses the json URL and turns data into lists.\n",
    "    3. process_image_urls() - this method, used by process_data(), converts the image urls into embedded vectors using an input model. Default model is the ResNet50.\n",
    "        4. imageurl2embedding() - uses model to convert image URL.\n",
    "        5. transform_image() - gets image into correct shape for ResNet (easier to train than VGG)\n",
    "    6. process_q_or_a() - process a list of questions or answers using an in-house tokenizer than creates a bag of words model similair to the described implimentation by Agrawal et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_url, count, q_tokenizer, a_tokenizer):\n",
    "    \"\"\"\n",
    "    MAJOR PREPROCESS METHOD.\n",
    "    \n",
    "    This processes a train, validate, or test annotation file\n",
    "    and saves the results as a local pickle object. \n",
    "    \n",
    "    This allows for caching, so embeddings only have to be made once per size.\n",
    "    \"\"\"\n",
    "    # type\n",
    "    data_type = data_url.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "    # turn json info into lists.\n",
    "    print(\"Processing json \\n\")\n",
    "    raw_imgurl_list, raw_questions_list, raw_answers_list = data2rawlists(data_url, count)\n",
    "    print(\"\\t DONE \\n\\n\")\n",
    "    \n",
    "    # process questions and answers.\n",
    "    print(\"Making Tokenizers \\n\")\n",
    "    ## use Q Tokenizers\n",
    "    processed_qs = process_q_or_a(raw_questions_list, q_tokenizer)\n",
    "    ## use A Tokenizers\n",
    "    processed_as = process_q_or_a(raw_answers_list, a_tokenizer)    \n",
    "    answer_count = len(a_tokenizer.word2index.keys())\n",
    "    print(\"\\t DONE \\n\\n\")\n",
    "    \n",
    "    # process images\n",
    "    print(\"Making Images \\n\")\n",
    "    image_emb_file = f\"{data_type}_{count}_images.pickle\"\n",
    "    if exists(image_emb_file):\n",
    "        image_embeddings = pickle.load(open(image_emb_file, \"rb\" ))\n",
    "    else:\n",
    "        image_embeddings = process_image_urls(raw_imgurl_list)\n",
    "        pickle.dump(image_embeddings, open(image_emb_file, \"wb\" ))\n",
    "    print(\"\\t DONE \\n\\n\")\n",
    "    \n",
    "    return np.array(image_embeddings), np.array(processed_qs), np.array(processed_as), answer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a data set and return raw lists.\n",
    "def data2rawlists(data, N, img_dir=\"https://vizwiz.cs.colorado.edu//VizWiz_visualization_img/\"):\n",
    "    \"\"\" returns list of all data into lists \"\"\"\n",
    "    raw_imgurl_list, raw_questions_list, raw_answers_list = [], [], []\n",
    "    split_val_data = requests.get(data, allow_redirects=True)\n",
    "    data = split_val_data.json()\n",
    "    for index in range(N):\n",
    "        #index data\n",
    "        vq = data[index]\n",
    "        # get/store image URL\n",
    "        image_name = vq['image']\n",
    "        image_url = img_dir + image_name\n",
    "        raw_imgurl_list.append(image_url)\n",
    "        # get/store Q string\n",
    "        question = vq['question']\n",
    "        raw_questions_list.append(question)\n",
    "        # get/store answers\n",
    "        if 'answers' in vq:\n",
    "            answers = vq['answers']\n",
    "            label = answers[0]['answer']\n",
    "            raw_answers_list.append(label)\n",
    "    return raw_imgurl_list, raw_questions_list, raw_answers_list \n",
    "\n",
    "# example usage\n",
    "# raw_imgurl_list, raw_questions_list, raw_answers_list = data2rawlists(data, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = resnet50.ResNet50()\n",
    "\n",
    "def process_image_urls(img_list):\n",
    "    \"\"\"\n",
    "    processes list of images into list of embeddings\n",
    "    \"\"\"\n",
    "    img_emb = []\n",
    "    for count, img in enumerate(img_list):\n",
    "        img_emb.append(imageurl2embedding(img)[0])\n",
    "        if count % 10 == 0:\n",
    "            print(f\"working on #: {count}\")\n",
    "    return img_emb\n",
    "\n",
    "def imageurl2embedding(unprocess_img_url, model=resnet_model):\n",
    "    \"\"\"\n",
    "    uses model to make embedding\n",
    "    \"\"\"\n",
    "    x = transform_image(unprocess_img_url)\n",
    "    embedding = model.predict(x)\n",
    "    return embedding\n",
    "\n",
    "def transform_image(unprocess_img_url, new_shape=(224, 224)):\n",
    "    \"\"\"\n",
    "    gets image into correct shape for ResNet (easier to train than VGG)\n",
    "    \"\"\"\n",
    "    img = io.imread(unprocess_img_url)\n",
    "#     visualize_image(unprocess_img_url)\n",
    "    x = cv2.resize(img, new_shape)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = resnet50.preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_q_or_a(q_a_list, tokenizer):\n",
    "    \"\"\"\n",
    "    processes list of questions or answers.\n",
    "    \"\"\"\n",
    "    string_emb_list = []\n",
    "    for count, string in enumerate(q_a_list):\n",
    "        string_emb_list.append(tokenizer.tokenize(string))\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"working on #: {count}\")\n",
    "    return string_emb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Steps\n",
    "\n",
    "Depending on whether or not it has been ran for the images, this step may be slow or fast. It's suggested to run with a low amount first to see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding top words\n",
      "\t DONE!\n",
      "making mappings\n",
      "\t DONE!\n",
      "finding top words\n",
      "\t DONE!\n",
      "making mappings\n",
      "\t DONE!\n",
      "Processing json \n",
      "\n",
      "\t DONE \n",
      "\n",
      "\n",
      "Making Tokenizers \n",
      "\n",
      "working on #: 0\n",
      "working on #: 1000\n",
      "working on #: 2000\n",
      "working on #: 3000\n",
      "working on #: 4000\n",
      "working on #: 5000\n",
      "working on #: 6000\n",
      "working on #: 7000\n",
      "working on #: 8000\n",
      "working on #: 9000\n",
      "working on #: 0\n",
      "working on #: 1000\n",
      "working on #: 2000\n",
      "working on #: 3000\n",
      "working on #: 4000\n",
      "working on #: 5000\n",
      "working on #: 6000\n",
      "working on #: 7000\n",
      "working on #: 8000\n",
      "working on #: 9000\n",
      "\t DONE \n",
      "\n",
      "\n",
      "Making Images \n",
      "\n",
      "\t DONE \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Training Data\n",
    "data_url = \"https://ivc.ischool.utexas.edu/VizWiz_final/vqa_data/Annotations/train.json\"\n",
    "\n",
    "number_of_training = 10000\n",
    "# training data used to create tokenizer\n",
    "raw_imgurl_list, raw_questions_list, raw_answers_list = data2rawlists(data_url, number_of_training)\n",
    "q_tokenizer = QuestionTokenizer(raw_questions_list, raw_answers_list, g_BAGOFWORDS_SIZE)\n",
    "a_tokenizer = AnswerTokenizer(raw_answers_list, len(raw_answers_list))\n",
    "\n",
    "train_processed_img, train_processed_q, train_processed_a, train_answers_count = process_data(data_url, number_of_training, q_tokenizer, a_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1000)\n",
      "(10000, 2000)\n",
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(train_processed_img.shape)\n",
    "print(train_processed_q.shape)\n",
    "print(train_processed_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing json \n",
      "\n",
      "\t DONE \n",
      "\n",
      "\n",
      "Making Tokenizers \n",
      "\n",
      "working on #: 0\n",
      "working on #: 1000\n",
      "working on #: 0\n",
      "working on #: 1000\n",
      "\t DONE \n",
      "\n",
      "\n",
      "Making Images \n",
      "\n",
      "\t DONE \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess validation Data\n",
    "data_url = \"https://ivc.ischool.utexas.edu/VizWiz_final/vqa_data/Annotations/val.json\"\n",
    "val_processed_img, val_processed_q, val_processed_a, val_answers_count = process_data(data_url, 2000, q_tokenizer, a_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000)\n",
      "(2000, 2000)\n",
      "(2000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(val_processed_img.shape)\n",
    "print(val_processed_q.shape)\n",
    "print(val_processed_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing json \n",
      "\n",
      "\t DONE \n",
      "\n",
      "\n",
      "Making Tokenizers \n",
      "\n",
      "working on #: 0\n",
      "\t DONE \n",
      "\n",
      "\n",
      "Making Images \n",
      "\n",
      "\t DONE \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Testing Data\n",
    "data_url = \"https://ivc.ischool.utexas.edu/VizWiz_final/vqa_data/Annotations/test.json\"\n",
    "test_processed_img, test_processed_q, test_processed_a, test_answers_count = process_data(data_url, 1000, q_tokenizer, a_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models.\n",
    "\n",
    "Here 3 different VQA models are created. Note that the Image Channel has beem processed by this point using a \"frozen\" copy of ResNet50, and the questions are really just a bag-of-words model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Multiply, Concatenate, Average\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras import Input, Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_68\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_155 (InputLayer)         [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " input_156 (InputLayer)         [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_266 (Dense)              (None, 1000)         1001000     ['input_155[0][0]']              \n",
      "                                                                                                  \n",
      " dense_267 (Dense)              (None, 1000)         2001000     ['input_156[0][0]']              \n",
      "                                                                                                  \n",
      " multiply_22 (Multiply)         (None, 1000)         0           ['dense_266[0][0]',              \n",
      "                                                                  'dense_267[0][0]']              \n",
      "                                                                                                  \n",
      " dense_268 (Dense)              (None, 1000)         1001000     ['multiply_22[0][0]']            \n",
      "                                                                                                  \n",
      " dense_269 (Dense)              (None, 10000)        10010000    ['dense_268[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,013,000\n",
      "Trainable params: 14,013,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL 1\n",
    "\n",
    "# get input ready\n",
    "img_in_layer = Input(shape=(g_EMBIMG_SIZE,))\n",
    "bow_in_layer = Input(shape=(g_BAGOFWORDS_SIZE,))\n",
    "# send through FF\n",
    "EMBEDDING_SIZE = 1000\n",
    "img_in_emb_layer = Dense(EMBEDDING_SIZE, activation=\"tanh\")(img_in_layer)\n",
    "bow_in_emb_layer = Dense(EMBEDDING_SIZE, activation=\"tanh\")(bow_in_layer)\n",
    "# combine output from first FFs\n",
    "mix_layer = Multiply()([img_in_emb_layer, bow_in_emb_layer ])\n",
    "# FF on combined information\n",
    "linear_layer = Dense(1000, activation=\"tanh\")(mix_layer)\n",
    "out_layer = Dense(train_processed_a.shape[1], activation=softmax)(linear_layer)\n",
    "\n",
    "\n",
    "# create model\n",
    "model_1 = Model(inputs=[img_in_layer, bow_in_layer], outputs=out_layer)\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_69\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_157 (InputLayer)         [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " input_158 (InputLayer)         [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_270 (Dense)              (None, 1000)         1001000     ['input_157[0][0]']              \n",
      "                                                                                                  \n",
      " dense_271 (Dense)              (None, 1000)         2001000     ['input_158[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 2000)         0           ['dense_270[0][0]',              \n",
      "                                                                  'dense_271[0][0]']              \n",
      "                                                                                                  \n",
      " dense_272 (Dense)              (None, 1000)         2001000     ['concatenate_20[0][0]']         \n",
      "                                                                                                  \n",
      " dense_273 (Dense)              (None, 10000)        10010000    ['dense_272[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,013,000\n",
      "Trainable params: 15,013,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL 2\n",
    "\n",
    "# get input ready\n",
    "img_in_layer = Input(shape=(g_EMBIMG_SIZE,))\n",
    "bow_in_layer = Input(shape=(g_BAGOFWORDS_SIZE,))\n",
    "# send through FF\n",
    "img_in_emb_layer = Dense(EMBEDDING_SIZE, activation=\"tanh\")(img_in_layer)\n",
    "bow_in_emb_layer = Dense(EMBEDDING_SIZE, activation=\"tanh\")(bow_in_layer)\n",
    "# combine output from first FFs\n",
    "mix_layer = Concatenate(axis=1)([img_in_emb_layer, bow_in_emb_layer ])\n",
    "# FF on combined information\n",
    "linear_layer = Dense(1000, activation=\"tanh\")(mix_layer)\n",
    "out_layer = Dense(train_processed_a.shape[1], activation=softmax)(linear_layer)\n",
    "\n",
    "# create model\n",
    "model_2 = Model(inputs=[img_in_layer, bow_in_layer], outputs=out_layer)\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_70\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_159 (InputLayer)         [(None, 1000)]       0           []                               \n",
      "                                                                                                  \n",
      " input_160 (InputLayer)         [(None, 2000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_274 (Dense)              (None, 1000)         1001000     ['input_159[0][0]']              \n",
      "                                                                                                  \n",
      " dense_275 (Dense)              (None, 1000)         2001000     ['input_160[0][0]']              \n",
      "                                                                                                  \n",
      " average_24 (Average)           (None, 1000)         0           ['dense_274[0][0]',              \n",
      "                                                                  'dense_275[0][0]']              \n",
      "                                                                                                  \n",
      " dense_276 (Dense)              (None, 1000)         1001000     ['average_24[0][0]']             \n",
      "                                                                                                  \n",
      " dense_277 (Dense)              (None, 10000)        10010000    ['dense_276[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,013,000\n",
      "Trainable params: 14,013,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODEL 3\n",
    "\n",
    "# get input ready\n",
    "img_in_layer = Input(shape=(g_EMBIMG_SIZE,))\n",
    "bow_in_layer = Input(shape=(g_BAGOFWORDS_SIZE,))\n",
    "# send through FF\n",
    "img_in_emb_layer = Dense(EMBEDDING_SIZE, activation=\"tanh\")(img_in_layer)\n",
    "bow_in_emb_layer = Dense(EMBEDDING_SIZE, activation=\"tanh\")(bow_in_layer)\n",
    "# combine output from first FFs\n",
    "mix_layer = Average()([img_in_emb_layer, bow_in_emb_layer ])\n",
    "# FF on combined information\n",
    "linear_layer = Dense(1000, activation=\"tanh\")(mix_layer)\n",
    "out_layer = Dense(train_processed_a.shape[1], activation=softmax)(linear_layer)\n",
    "\n",
    "\n",
    "# create model\n",
    "model_3 = Model(inputs=[img_in_layer, bow_in_layer], outputs=out_layer)\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Save Models\n",
    "While training, the loss information is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "3/3 [==============================] - 9s 2s/step - loss: 12.0640 - accuracy: 0.1345 - val_loss: 11.0448 - val_accuracy: 0.3480\n",
      "Epoch 2/40\n",
      "3/3 [==============================] - 6s 2s/step - loss: 12.0539 - accuracy: 0.3357 - val_loss: 11.0282 - val_accuracy: 0.3565\n",
      "Epoch 3/40\n",
      "3/3 [==============================] - 6s 2s/step - loss: 12.0330 - accuracy: 0.3400 - val_loss: 10.9966 - val_accuracy: 0.3575\n",
      "Epoch 4/40\n",
      "3/3 [==============================] - 4s 1s/step - loss: 11.9951 - accuracy: 0.3401 - val_loss: 10.9434 - val_accuracy: 0.3575\n",
      "Epoch 5/40\n",
      "3/3 [==============================] - 4s 1s/step - loss: 11.9328 - accuracy: 0.3403 - val_loss: 10.8567 - val_accuracy: 0.3575\n",
      "Epoch 6/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 11.8316 - accuracy: 0.3403 - val_loss: 10.7139 - val_accuracy: 0.3575\n",
      "Epoch 7/40\n",
      "3/3 [==============================] - 5s 1s/step - loss: 11.6665 - accuracy: 0.3403 - val_loss: 10.4734 - val_accuracy: 0.3575\n",
      "Epoch 8/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 11.3958 - accuracy: 0.3403 - val_loss: 10.0810 - val_accuracy: 0.3575\n",
      "Epoch 9/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 10.9782 - accuracy: 0.3403 - val_loss: 9.5466 - val_accuracy: 0.3575\n",
      "Epoch 10/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 10.4761 - accuracy: 0.3403 - val_loss: 9.1078 - val_accuracy: 0.3575\n",
      "Epoch 11/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 10.1356 - accuracy: 0.3403 - val_loss: 9.1096 - val_accuracy: 0.3575\n",
      "Epoch 12/40\n",
      "3/3 [==============================] - 7s 2s/step - loss: 10.2425 - accuracy: 0.3403 - val_loss: 9.4997 - val_accuracy: 0.3575\n",
      "Epoch 13/40\n",
      "3/3 [==============================] - 12s 3s/step - loss: 10.6008 - accuracy: 0.3403 - val_loss: 9.6667 - val_accuracy: 0.3575\n",
      "Epoch 14/40\n",
      "3/3 [==============================] - 6s 2s/step - loss: 10.7222 - accuracy: 0.3403 - val_loss: 9.4756 - val_accuracy: 0.3575\n",
      "Epoch 15/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 10.6160 - accuracy: 0.3403 - val_loss: 9.3754 - val_accuracy: 0.3575\n",
      "Epoch 16/40\n",
      "3/3 [==============================] - 5s 2s/step - loss: 10.5503 - accuracy: 0.2400 - val_loss: 9.4005 - val_accuracy: 0.1700\n",
      "Epoch 17/40\n",
      "3/3 [==============================] - 7s 3s/step - loss: 10.5645 - accuracy: 0.2600 - val_loss: 9.4882 - val_accuracy: 0.3575\n",
      "Epoch 18/40\n",
      "3/3 [==============================] - 6s 2s/step - loss: 10.6628 - accuracy: 0.3403 - val_loss: 9.5634 - val_accuracy: 0.3575\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4f/nzw4pyy13p90pvc1lkbpbjpc0000gn/T/ipykernel_11060/3785380079.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     model.fit(x=[train_processed_img, train_processed_q], y=train_processed_a, \n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_processed_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_processed_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_processed_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 epochs=EPOCHS, batch_size=4000)\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/neural/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=0.001, decay=1e-3 / 200)\n",
    "EPOCHS = 40\n",
    "for model in [model_1, model_2, model_3]:\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.fit(x=[train_processed_img, train_processed_q], y=train_processed_a, \n",
    "                validation_data=([val_processed_img, val_processed_q], val_processed_a),\n",
    "                epochs=EPOCHS, batch_size=4000)\n",
    "    \n",
    "    # create plot illustrating the training history\n",
    "    plt.title(f\"Training Loss\")\n",
    "    plt.plot(model.history.history['loss'], label='training loss', color='orange')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    plt.title(f\"Validation Loss\")\n",
    "    plt.plot(model.history.history['val_loss'], label='validation loss')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# obtain accuracy on the validation data\n",
    "\n",
    "Here the metric is based on the crowd agreement, per se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(prediction_list, true_raw, answer_tokenizer=a_tokenizer):\n",
    "    \"\"\"\n",
    "    This method scores the output prediction\n",
    "    against the true raw answers\n",
    "    \"\"\"\n",
    "    accuracy_array = []\n",
    "    # loop through prediction, detokenize\n",
    "    for index, pred in enumerate(prediction_list):\n",
    "        word = answer_tokenizer.index2word[np.argmax(pred)]\n",
    "        true_list = true_raw[index]\n",
    "#         print(word, true_list)\n",
    "        accuracy_array.append(np.minimum(1.0, true_list.count(word)/3.0))\n",
    "    return accuracy_array\n",
    "\n",
    "def get_predictions(prediction_list, answer_tokenizer=a_tokenizer):\n",
    "    \"\"\"\n",
    "    returns predicted words\n",
    "    \"\"\"\n",
    "    predicted_words = []\n",
    "    # loop through prediction, detokenize\n",
    "    for index, pred in enumerate(prediction_list):\n",
    "        word = answer_tokenizer.index2word[np.argmax(pred)]\n",
    "        predicted_words.append(word)\n",
    "    return predicted_words\n",
    "\n",
    "def url2truelist(data):\n",
    "    \"\"\"\n",
    "    This method obtains a list of lists portraying \n",
    "    the characterized image labels.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    for x in data:\n",
    "        tmp = []\n",
    "        for y in x['answers']:\n",
    "            tmp.append(y['answer'])\n",
    "        answers.append(tmp)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://ivc.ischool.utexas.edu/VizWiz_final/vqa_data/Annotations/val.json\"\n",
    "split_val_data = requests.get(data_url, allow_redirects=True)\n",
    "data = split_val_data.json()\n",
    "true_list = url2truelist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4916666666666667\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "9367",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4f/nzw4pyy13p90pvc1lkbpbjpc0000gn/T/ipykernel_11060/976657395.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprediction_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_processed_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_processed_q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4f/nzw4pyy13p90pvc1lkbpbjpc0000gn/T/ipykernel_11060/2718663927.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(prediction_list, true_raw, answer_tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# loop through prediction, detokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtrue_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         print(word, true_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 9367"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in [model_1, model_2, model_3]:\n",
    "    prediction_list = model.predict([np.array(train_processed_img[:200]), np.array(train_processed_q[:200])])\n",
    "    score_array = np.average(score(prediction_list, true_list))\n",
    "    print(score_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for first 1000 samples of testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_1 # best model\n",
    "prediction_list = model.predict([np.array(test_processed_img), np.array(test_processed_q)])\n",
    "prediction_list = get_predictions(prediction_list)\n",
    "df = pd.DataFrame(prediction_list)\n",
    "df.to_csv(\"results.csv\", header = None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
